# Consistent Hashing
* 보편적인 해싱 방식인, hashCode % n (n = 해시 버킷 크기) 은 분산 캐시 서버에 적용하기엔 확장성이 떨어진다.
  * 분산 캐시 서버의 경우, n 이 서버 개수가 된다.
  * 서버가 추가되거나, 장애가 발생하여 서비스에서 제외될경우, n 의 값이 달라진다.
  * 그로인해, 전체 캐시 entry의 해싱 결과값이 달라져 대량의 cache miss 가 발생하게 되고 Origin 서버로 급격한 부하가 전파된다.
* 캐시 서버의 추가나 삭제에 상관없이 항상 일관된 해싱 결과값을 보장하여 Scailabilty 한 캐시 서버 구축을 가능하게 해주는 기술이 Consistent Hashing 이다.

### 원리
1. 먼저, SHA-1 이나 MurmurHash 와 같은 균등 분포 해싱 알고리즘을 이용하여, 각 캐시 서버를 해시링위에 균등한 간격으로 위치시킨다.

<img width="618" alt="image" src="https://github.com/JisooOh94/study/assets/48702893/97b80e4b-d2be-4b10-81d6-414150901607">

<img width="685" alt="image" src="https://github.com/JisooOh94/study/assets/48702893/d3f43301-b3a5-45f1-8a53-31b241411169">

2. 캐시 조회 요청이 들어올경우, 마찬가지로 균등 분포 해싱 알고리즘을 이용하여 캐시키를 해싱한다. 해싱 결과값을 해시링위에 위치시킨다.
3. 해시링 위에서 시계방향으로 순회하며 가장 첫번째로 만나는 캐시서버에 캐싱한다.

<img width="595" alt="image" src="https://github.com/JisooOh94/study/assets/48702893/4c22dd20-469a-4893-b7cb-ea2526f8b220">

4. 서버가 추가되거나, 장애가 발생하여 해시링에서 제외되는경우, 해시링 위에서 변경이 발생한 캐시 서버로부터 반시계 방향에 있는 캐시들만 재배치가 이루어진다.

<img width="653" alt="image" src="https://github.com/JisooOh94/study/assets/48702893/ef5cfe86-0034-4771-b3d0-3bbc729bb700">


### 예시 코드

```java
import org.apache.commons.codec.digest.DigestUtils;
import java.nio.ByteBuffer;
import java.util.SortedMap;
import java.util.TreeMap;

public class ConsistentHashing {
    // SortedMap을 사용하여 해시 값을 키로 하고 노드 이름을 값으로 저장하는 해시 링을 구현
    private final SortedMap<Integer, String> circle = new TreeMap<>();
    private final int numberOfReplicas; // 각 노드의 복제본 수

    public ConsistentHashing(int numberOfReplicas) {
        this.numberOfReplicas = numberOfReplicas;
    }

    // 주어진 키에 대해 MD5 해시를 계산하고, 이를 정수로 변환하여 반환
    private int hash(String key) {
        byte[] digest = DigestUtils.md5(key);
        // ByteBuffer를 사용하여 바이트 배열을 정수로 변환
        return ByteBuffer.wrap(digest).getInt();
    }

    // 노드를 해시 링에 추가
    public void addNode(String node) {
        for (int i = 0; i < numberOfReplicas; i++) {
            // 각 노드에 대해 복제본을 만들고, 이를 해시 링에 추가
            circle.put(hash(node + i), node);
        }
    }

    // 해시 링에서 노드를 제거
    public void removeNode(String node) {
        for (int i = 0; i < numberOfReplicas; i++) {
            // 각 노드의 복제본을 해시 링에서 삭제
            circle.remove(hash(node + i));
        }
    }

    // 주어진 키에 대해 적절한 노드를 찾음
    public String getNode(String key) {
        if (circle.isEmpty()) {
            return null; // 해시 링이 비어있으면 null 반환
        }
        int hash = hash(key);
        // 주어진 해시 값이 링에 존재하지 않을 경우 처리
        if (!circle.containsKey(hash)) {
            // 주어진 해시 값보다 크거나 같은 모든 키-값 쌍을 포함하는 부분 맵을 가져옴
            SortedMap<Integer, String> tailMap = circle.tailMap(hash);

            // tailMap이 비어 있을 경우, 링의 처음으로 돌아가 첫 번째 키를 선택
            // 그렇지 않으면, tailMap의 첫 번째 키를 선택
            hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey();
        }
        // 결정된 해시 값을 사용하여 해당 노드를 반환
        return circle.get(hash);
    }
}
```

### 한계점
* 캐시 서버가 추가되거나 삭제되는 경우, 1의 과정에서 수행했던 해시링 위의 각 캐시 서버의 균등한 간격이 깨지게 된다.
* 이로인해 캐시의 균등 분포 또한 깨지게 된다. 결국 특정 캐시 서버에만 캐싱이 몰리는 문제가 발생한다.

### 한계점 극복
* 실제 캐시 서버를 가리키는 가상 노드들을, 해시링위에 마찬가지로 균등한 간격으로 위치시킨다.
* 가상 노드가 많아질수록, 해시링 위의 캐시 서버들(가상 노드 포함) 사이 간격이 좁아져 새로운 캐시 서버가 추가되거나 삭제되어도 영향받는 캐시의 수가 작아진다. 이를 통해 캐시 서버의 추가나 삭제에 영향 없이 균등 분포를 보장해준다.
  * 하지만 가상 노드가 많아질수록 저장해야할 가상 노드 정보도 많아져 메모리 사용량이 높아진다. 운영을 해보며 적절한 값으로 타협
* 가상 노드를 통해 hotspot key 문제 또한 완화가 가능하다.
  * 가상 노드가 많아질수록, 요청량이 많은 캐시들이 하나의 캐시 서버에 캐싱되지 않고 여러 캐시 서버로 분배되어 저장될 확률도 높아진다.

> Ref
> * https://honglab.tistory.com/251
