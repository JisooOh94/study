# URL 단축기

## 1. 요구사항 파악
* 다음의 질문들이 가능하다
  * 단축 URL 생성 요청 및 조회 요청의 트래픽 양
  * 단축 URL 길이 및 단축 URL 에 허용된 문자
  * 단축 URL retention 기간

* 필요한 기능은 다음과 같다
  * 단축 URL 생성 및 저장
  * 단축 URL 에 해당하는 원본 URL 조회

* 개략적 추정은 다음과 같다
  * 일평균 단축 URL 생성 요청은 1억건, 조회요청은 그 10배, 단축 URL 에 허용된 문자는 알파벳 + 숫자만 허용, retention 기간은 10년으로 가정
  * 평균 TPS
    * 생성 요청 : 1160
    * 조회 요청 : 11600
  * 필요한 데이터 storage 크기
    * retention 기간인 10년동안 생성될 단축 URL 개수 : 1억 * 365 * 10 = 3650억개
    * 단축 URL 에 허용된 문자 종류 개수는 62개(알파벳 개수 + 숫자 0 ~ 9)
    * 따라서, 가능한 최소 단축 URL 길이는 7
      * 3650억개의 단축 URL 간 중복이 발생하면 안된다
      * 따라서 62^n >= 3650 이 되는 최소 n 의 값이 가능한 최소 단축 URl 길이
    * 단축 URL 길이를 7로 했을때 collision 확률 계산
      * 정확한 collision 확률 계산 공식은 다음과 같다.
      ```java
      //n - 버킷 개수, k - 데이터 개수
      1 - 2n! / (2kn (2n - k)!)
      ```
      * 하지만 위 공식을 실제로 사용하기엔 복잡하고 오버헤드도 크다. 따라서 실무에선 보통 birthday problem 방식으로 해시 충돌 확률을 추정한다.
        * birthday problem 은 버킷 개수가 n 일때, 충돌확률이 50% 가 되는 데이터 개수를 구하는 방식. 공식은 다음과 같다. `sqrt{2N * ln2}`
      * birthday problem 공식으로 계산시, 버킷 개수가 62^7 일때 데이터 개수가 약 220만개일 경우 collision 확률이 50% 가 된다. 
      ```java
      Math.sqrt(2 * N * Math.log(2))
      ```  
      * 따라서 데이터개수가 3650 억개일경우 collision 확률은 거의 100%에 가까울것으로 보인다...? 이 계산이 맞나?
    * 데이터 storage 에 단축 URL 과 원본 URL 만 저장한다고 가정한다면 필요한 데이터 storage 크기는 (7 byte + 100 byte) * 100000000 * 365 * 10 = 47450000000 * 10^{13} = 39 TB
      * 하지만 이값은 단축 URL 길이를 최소값인 7로 생성한다는 가정, 그리고 원본 URL 및 단축 URL 만 저장한다는 가정하게 산정한 storage 크기다
      * storage 에 URL 외에 생성 날짜나 unique id 를 함께 저장하거나 해싱 알고리즘이 생성하는 단축 URL 길이에 따라 필요한 storage 크기가 더 커질 수 있다 

## 2. 개략적 설계
### 1. 보편적 해싱 알고리즘 사용
* SHA-1, MD5 와 같은 해싱 알고리즘을 사용하여 url 해싱
* 3650 억개의 데이터를 낮은 collision 확률로 해싱을 하기 위해선 해시 결과값(해시 버킷 수)이 길어질 수 밖에 없다. (birthday problem)
  * SHA-1 같은경우, 160 bit 의 해시코드를 생성하고 이를 16진수로 파싱하면 40개의 16진수 문자로 이루어진 해시코드가 된다. collision 확률은 낮아지겠지만 단축 url 이라하기엔 너무 길다.
* 따라서 생성된 16진수 해시코드에서 최소 가능한 단축 url 길이인 7개의 코드만 사용하는 방식으로 개선 가능하다
  * 그러나, 이렇게 될경우 collision 확률이 매우 높아진다.
* 이를 또 해결하기 위해, 
  1) DB 조회하여 중복되는 단축 url이 있는지 검사하고, 
  2) 있다면 원본 url 뒤에 미리 정의한 문자를 붙인뒤 다시 해싱을 통해 단축 url 생성, 
  3) 중복이 발생안할때까지 1,2 과정을 반복 하는 방식이 있으나 매우 비효율적 (BloomFilter 를 이용해 DB IO 를 줄이는방법이 있으나 또다른 문제 발생)

### 2. 유일 ID 생성기 및 base-62 변환
* 유일 ID 를 base62 로 인코딩하여 단축 url 을 생성하는 방식으로 과정은 다음과 같다.
  1. 단축 url 생성 요청시, 요청에 대한 유일 ID 생성
    * base62 로 인코딩시 최대길이가 7이어야 하므로 유일 ID 의 크기도 62^7(약 13자리) 이하여야함
    * 따라서 timestamp 기반의 unique id 사용 불가능, 별도의 counter 를 이용한 unique id 생성
    * original url : https://foo/bar
    * unique ID : 1000000000001
  3. 생성된 유일 ID 를 base-62 로 인코딩하여 단축 url 생성
     * encoded url : HbXm5a5
  4. 단축 url, 원본 url 을 DB 에 저장후 단축 url 반환

## 3. 상세 설계
### 서버 구동

![image](https://github.com/JisooOh94/study/assets/48702893/18117855-cf94-4720-b735-14a37f3f6d5a)

* timestamp 기반 uniqueId 는 사용할 수 없으므로, 0 ~ 62^7 범위의 숫자를 uniqueId 로 사용한다.
  * 10년동안 생성될 전체 단축 url 간 중복을 일으키지 않으면서도 base62 인코딩한 단축 Url 최대 길이가 7 이하임을 보장할 수 있는 uniqueId
* 단축 url 서버 클러스터는 n개의 서버로 분산 구성되어있다. 각 서버에서 단축 url 생성에 사용할 uniqueId 간 중복을 방지하기 위해 각 서버마다 사용할 수 있는 uniqueId 범위를 할당한다.
* 이를 위해, 서버 구동시점에 Offset Redis 로부터 start offset 조회 및 Range 크기 만큼 increament 한다.
  * 각 서버마다 10000 개의 unique id 를 사용할 수 있다고 설정하면 Range 크기는 10000 이 된다.
  * Offset Redis 는 단축 url 서버 클러스터에 신규 서버가 추가 될때만 조회된다. 따라서 Redis cluster 까진 필요 없을듯하다. Failover 정도만 가능한 Redis Sentinel 로 구축

> Redis 는 영속적인 데이터 저정소보단 빠른 조회속도를 보장하는 캐시 저장소 목적의 모듈이기 때문에 위와같은 용도로 사용할땐 Redis 보단 Zookeeper 와 같은 분산 시스템 코딘네이터가 더 적절할 수 있다. (하지만 설계 및 구현하기엔 redis 가 더 간단하므로 채택)

### 단축 URL 생성 요청

![image](https://github.com/JisooOh94/study/assets/48702893/f450798e-7630-43cc-941e-b518edd911e1)

* 각 서버는 단축 URL 생성 요청 인입시, Counter 를 1씩 증가시켜가며 단축 url 생성에 사용할 uniqueId 를 발급받는다.
  * 예를들어, Range 크기를 10000 으로 설정하고 첫번째 단축 url 서버가 start offset 을 0 을 할당받았다면, 
    * counter 를 0으로 초기화후, 단축 url 생성 요청 인입시마다 counter 값을 unique Id 로 발급후 +1 한다.
    * counter 값이 Range 크기인 10000 까지 증가했다면, Offset Redis 로부터 새로운 start offset 을 조회하여 update 한다.
* 발급받은 uniqueId 를 base62 인코딩 하여 단축 url 로 파싱후 NoSql DB 에 key(단축 url) - value(원본 url) 로 저장 및 클라이언트로 응답한다.
  * 원본 url 조회 요청 처리시, 복잡한 스키마나 Join 연산 필요 없이 단순히 단축 url 에 매칭되는 원본 url 만 검색하면 되므로 조회 속도도 빠르고 확장성도 좋은 NoSql DB 를 사용하는것이 적절해보인다.

### 원본 URL 조회

![image](https://github.com/user-attachments/assets/409e0834-0fec-4dc5-8ea5-700a4ea76b73)

* 단축 URL 시스템은 일반적으로 단축 URL 생성 요청보단 단축 URL 에 해당하는 원본 URL 조회 요청이 훨씬 많은, Read heavy 한 서비스이다. 따라서 원본 URL 조회 처리 성능이 중요하다.
* 클라이언트로부터 원본 URL 조회 요청 인입시, 단축 URL 서버는 NoSql DB Cluster 로부터 단축 URL 에 매칭되는 원본 URL 을 조회하여 응답한다.
  * 그러나, 아무리 조회 속도가 빠른 NoSql DB 를 샤딩까지 하여 부하분산 까지 해준다 해도, 요청량에 따라 병목이 발생할 수 있다.
  * 따라서 자주 요청되는 원본 URL 은 local 캐시에 저장해두고 캐시를 통해 조회해가게 하여 성능 향상 시킨다.
    * 보편적인 원본 URL 조회 요청 패턴은 몇몇 원본 URL 이 대부분의 조회 요청 비중을 차지하는 패턴을 보인다. (e.g. www.google.com/main, www.naver.com/main)
* Redis 의 Top-K cache 를 이용해 요청량에 따른 원본 URL 을 정렬하여 보관하고, 주기적으로 그중 상위 k 개만 각 단축 URL 서버의 local cache 에 저장한다.
  * 원본 URL 조회 요청시 Redis pub-sub 구조를 이용해 Top-K Redis 에 요청횟수를 update 한다. Redis pub-sub 을 사용한 이유는
    1. 자주 요청되는 원본 URL 일 경우, Redis Top-K cache 에 자주 update 가 발생하여 hotspot key 문제가 발생할 수 있다. 따라서 MQ 를 이용해 update 요청을 쓰로틀링 하여 원본 URL 조회 요청 처리가 지연되지 않도록 한다.
    2. 요청횟수 update 요청의 경우 굳이 redis streams 나 kafka 와 같은 영속적 MQ 를 통한 at lease once semantic 을 보장할 필요가 없다. (일부 유실되어도 서비스 운영엔 크게 문제 없음). 따라서 휘발성 MQ 인 Reids pub-sub 을 사용한다.(In-Memory 방식이므로 속도 빠름)
  * 각 서버는 주기적으로 Redis Top-K cache 로부터 상위 k 개의 원본 url 을 조회하여 local cache 를 update 한다.
